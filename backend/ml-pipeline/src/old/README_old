# Machine learning system for Risk Assessment

This is a Machine Learning Model Risk Scoring and Monitoring project from my previous [github repository for Predictive Analytics for Business](https://github.com/cjakabos/portfolio-business-analytics), development will continue in this repository. The problem is to create, deploy, and monitor a risk assessment ML model that will estimate the attrition risk of each of the company's clients. Also setting up processes to re-train, re-deploy, monitor and report on the ML model.

## Prerequisites
- Python 3 required
- Virtualenv recommended

## Dependencies
This project dependencies is available in the ```requirements.txt``` file.

## Installation
Use the package manager [pip](https://pip.pypa.io/en/stable/) to install the dependencies from the ```requirements.txt```. Its recommended to install it in a separate [virtual environment](https://virtualenv.pypa.io/en/latest/).

## Project Structure
```bash
ðŸ“¦Dynamic-Risk-Assessment-System
 â”£
 â”£ ðŸ“‚data
 â”ƒ â”£ ðŸ“‚ingesteddata                 # Contains csv and metadata of the ingested data
 â”ƒ â”ƒ â”£ ðŸ“œfinaldata.csv
 â”ƒ â”ƒ â”— ðŸ“œingestedfiles.txt
 â”ƒ â”£ ðŸ“‚practicedata                 # Data used for practice mode initially
 â”ƒ â”ƒ â”£ ðŸ“œdataset1.csv
 â”ƒ â”ƒ â”— ðŸ“œdataset2.csv
 â”ƒ â”£ ðŸ“‚sourcedata                   # Data used for production mode
 â”ƒ â”ƒ â”£ ðŸ“œdataset3.csv
 â”ƒ â”ƒ â”— ðŸ“œdataset4.csv
 â”ƒ â”— ðŸ“‚testdata                     # Test data
 â”ƒ â”ƒ â”— ðŸ“œtestdata.csv
 â”£ ðŸ“‚model
 â”ƒ â”£ ðŸ“‚models                       # Models pickle, score, and reports for production mode
 â”ƒ â”ƒ â”£ ðŸ“œapireturns.txt
 â”ƒ â”ƒ â”£ ðŸ“œconfusionmatrix.png
 â”ƒ â”ƒ â”£ ðŸ“œlatestscore.txt
 â”ƒ â”ƒ â”£ ðŸ“œsummary_report.pdf
 â”ƒ â”ƒ â”— ðŸ“œtrainedmodel.pkl
 â”ƒ â”£ ðŸ“‚practicemodels               # Models pickle, score, and reports for practice mode
 â”ƒ â”ƒ â”£ ðŸ“œapireturns.txt
 â”ƒ â”ƒ â”£ ðŸ“œconfusionmatrix.png
 â”ƒ â”ƒ â”£ ðŸ“œlatestscore.txt
 â”ƒ â”ƒ â”£ ðŸ“œsummary_report.pdf
 â”ƒ â”ƒ â”— ðŸ“œtrainedmodel.pkl
 â”ƒ â”— ðŸ“‚production_deployment        # Deployed models and model metadata needed
 â”ƒ â”ƒ â”£ ðŸ“œingestedfiles.txt
 â”ƒ â”ƒ â”£ ðŸ“œlatestscore.txt
 â”ƒ â”ƒ â”— ðŸ“œtrainedmodel.pkl
 â”£ ðŸ“‚src
 â”ƒ â”£ ðŸ“œapicalls.py                  # Runs app endpoints
 â”ƒ â”£ ðŸ“œapp.py                       # Flask app
 â”ƒ â”£ ðŸ“œconfig.py                    # Config file for the project which depends on config.json
 â”ƒ â”£ ðŸ“œdeployment.py                # Model deployment script
 â”ƒ â”£ ðŸ“œdbtest.py                    # DB setup test script
 â”ƒ â”£ ðŸ“œdiagnostics.py               # Model diagnostics script
 â”ƒ â”£ ðŸ“œfullprocess.py               # Process automation
 â”ƒ â”£ ðŸ“œingestion.py                 # Data ingestion script
 â”ƒ â”£ ðŸ“œpretty_confusion_matrix.py   # Plots confusion matrix
 â”ƒ â”£ ðŸ“œreporting.py                 # Generates confusion matrix and PDF report
 â”ƒ â”£ ðŸ“œscoring.py                   # Scores trained model
 â”ƒ â”£ ðŸ“œtraining.py                  # Model training
 â”ƒ â”— ðŸ“œwsgi.py
 â”£ ðŸ“œconfig.json                    # Config json file
 â”£ ðŸ“œcronjob.txt                    # Holds cronjob created for automation
 â”£ ðŸ“œREADME.md
 â”— ðŸ“œrequirements.txt               # Projects required dependencies
```

## Steps Overview
1. **Data ingestion:** Automatically check if new data that can be used for model training. Compile all training data to a training dataset and save it to folder.
2. **Training, scoring, and deploying:** Write scripts that train an ML model that predicts attrition risk, and score the model. Saves the model and the scoring metrics.
3. **Diagnostics:** Determine and save summary statistics related to a dataset. Time the performance of some functions. Check for dependency changes and package updates.
4. **Reporting:** Automatically generate plots and PDF document that report on model metrics and diagnostics. Provide an API endpoint that can return model predictions and metrics.
5. **Process Automation:** Create a script and cron job that automatically run all previous steps at regular intervals.

<img src="images/fullprocess.jpg" width=550 height=300>

## Usage

### 0- Run Flask App in separate terminal, run the rest of the steps in another terminal
```bash
virtualenv venv
source venv/bin/active
pip3 install -r requirements.txt
python3 app.py
```

### 1- Edit config.json file to use practice data

```bash
"input_folder_path": "practicedata",
"output_folder_path": "ingesteddata",
"test_data_path": "testdata",
"output_model_path": "practicemodels",
"prod_deployment_path": "production_deployment"
```

### 2- Run data ingestion
```bash
cd src
virtualenv venv2
source venv2/bin/active
pip3 install -r requirements.txt
python3 ingestion.py
```
Artifacts output:
```
data/ingesteddata/finaldata.csv
data/ingesteddata/ingestedfiles.txt
```

### 3- Model training
```bash
virtualenv venv2
source venv2/bin/active
pip3 install -r requirements.txt
python3 training.py
```
Artifacts output:
```
models/practicemodels/trainedmodel.pkl
```

###  4- Model scoring
```bash
virtualenv venv2
source venv2/bin/active
pip3 install -r requirements.txt
python3 scoring.py
```
Artifacts output:
```
models/practicemodels/latestscore.txt
```

### 5- Model deployment
```bash
virtualenv venv2
source venv2/bin/active
pip3 install -r requirements.txt
python3 deployment.py
```
Artifacts output:
```
model/prod_deployment_path/ingestedfiles.txt
model/prod_deployment_path/trainedmodel.pkl
model/prod_deployment_path/latestscore.txt
```

### 6- Run diagnostics
```bash
virtualenv venv2
source venv2/bin/active
pip3 install -r requirements.txt
python3 diagnostics.py
```

### 7- Run reporting
```bash
python3 reporting.py
```
Artifacts output:
```
models/practicemodels/confusionmatrix.png
models/practicemodels/summary_report.pdf
```

### 8- Run API endpoints
```bash
virtualenv venv2
source venv2/bin/active
pip3 install -r requirements.txt
python3 apicalls.py
```
Artifacts output:
```
models/practicemodels/apireturns.txt
```

### 9- Edit config.json file to use production data

```bash
"input_folder_path": "sourcedata",
"output_folder_path": "ingesteddata",
"test_data_path": "testdata",
"output_model_path": "models",
"prod_deployment_path": "production_deployment"
```

### 10- Full process automation
```bash
virtualenv venv2
source venv2/bin/active
pip3 install -r requirements.txt
python3 fullprocess.py scripted
```
### 11- Cron job

Start cron service
```bash
sudo service cron start
```

Edit crontab file
```bash
sudo crontab -e
```
   - Select **option 3** to edit file using vim text editor
   - Press **i** to insert a cron job
   - Write the cron job in ```cronjob.txt``` which runs ```fullprocces.py``` every 10 mins
   - Save after editing, press **esc key**, then type **:wq** and press enter

View crontab file
```bash
sudo crontab -l
```

Moving from local csv files to DB version:
Install Postgres, on mac:

```bash
brew install postgresql@15
brew services start postgresql@15
echo 'export PATH=/opt/homebrew/opt/postgresql@15/bin/postgres:$PATH  ' >> ~/.zshrc
```

Start postgres and create riskdb

```
psql postgres

CREATE DATABASE riskdb;

CREATE USER riskmaster WITH PASSWORD 'apetite';

GRANT ALL ON DATABASE riskdb TO riskmaster;

ALTER DATABASE riskdb OWNER TO riskmaster;

GRANT ALL PRIVILEGES ON DATABASE riskdb TO riskmaster;

\c riskdb riskmaster

GRANT ALL ON SCHEMA public TO riskmaster;

exit
```

Example of sending a new customer request, this will triger the full Risk scoring pipeline:
```bash
curl -H "Content-Type: application/json" -X POST -d \
'{
    "fields": {
        "corporation": "Risky AB",
        "lastmonth_activity": 100,
        "lastyear_activity": 1200,
        "number_of_employees": 16,
        "exited": 0
    }
}' \
http://localhost:8000/ingest
```


Start postgres and create segmentationdb

```
psql postgres

CREATE DATABASE segmentationdb;

CREATE USER segmentmaster WITH PASSWORD 'segment';

GRANT ALL ON DATABASE segmentationdb TO segmentmaster;

ALTER DATABASE segmentationdb OWNER TO segmentmaster;

GRANT ALL PRIVILEGES ON DATABASE segmentationdb TO segmentmaster;

\c riskdb segmentmaster

GRANT ALL ON SCHEMA public TO segmentmaster;

exit
```